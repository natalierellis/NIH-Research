
#1. Using the mtcars data, fit a logistic regression model to distinguish
# 8-cyl cars from non-8-cyl cars using mpg and drat.
mtcars$new_cyl[mtcars$cyl == 8] = 1
mtcars$new_cyl[mtcars$cyl == 4] = 0
mtcars$new_cyl[mtcars$cyl == 6] = 0
modlog <- glm(new_cyl ~ mpg + drat, data = mtcars, family = binomial(link = "logit"))

#2. Does the model from #1 meet the log-odds linearity assumption??
mtcars$logodds = predict(modlog)
pairs(mtcars[, colnames(mtcars)%in%c("logodds", "mpg", "drat")])
#check for linear relationship between log0odds and predictors

#3. Interpret model output (significant predictors) from the model in #1
summary(modlog)
#mpg is a sign. predictor, as it increases, cars are less likely to be 8 cylinder

#4. Create a confusion matrix of model predictions.  How accurate are model predictions?
preds = predict(modlog, type = "response")
predsbin = ifelse(preds>0.5, 1, 0)
table(mtcars$new_cyl, predsbin)

#Very accurate model because only 2 misclassifications 
#5. Using the mtcars data, fit a logistic regression model to distinguish
# 8-cyl cars from non-8-cyl cars using mpg, drat, and carb.
modlog2 <- glm(new_cyl ~ mpg + drat + carb, data = mtcars, family = binomial(link = "logit"))
summary(modlog2)
#6. Does the model from #5 have better or worse fit than the model from #1?
#New Model has worse fit because AIC number is higher 

#7. Does the model from #5 have better or worse predictive accuracy than the model from #1?
preds2 = predict(modlog2, type = "response")
predsbin2 = ifelse(preds2>0.5, 1, 0)
table(mtcars$new_cyl, predsbin2)
#The new model has less predictive accuracy as well 
